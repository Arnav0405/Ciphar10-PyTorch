{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x161692da3c0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "# import numpy as np\n",
    "import cupy as cp\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from random import random\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.device_count()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Data\n",
    "Understanding the file structure and the structuring of the given dataset by kaggle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "IMG_HEIGHT = 32\n",
    "IMG_WIDTH = 32\n",
    "RANDOM_SEED = 28\n",
    "\n",
    "root = Path(\"data/cifar10\")\n",
    "\n",
    "# (root/'train').rename(root/'original_train')\n",
    "# (root/'test').rename(root/'original_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = r\"data/cifar10/original_train/train\"\n",
    "csv_path = r\"data/cifar10/trainLabels.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id       label\n",
      "0   1        frog\n",
      "1   2       truck\n",
      "2   3       truck\n",
      "3   4        deer\n",
      "4   5  automobile\n",
      "{'frog': 0, 'truck': 1, 'deer': 2, 'automobile': 3, 'bird': 4, 'horse': 5, 'ship': 6, 'cat': 7, 'dog': 8, 'airplane': 9}\n",
      "{0: 'frog', 1: 'truck', 2: 'deer', 3: 'automobile', 4: 'bird', 5: 'horse', 6: 'ship', 7: 'cat', 8: 'dog', 9: 'airplane'}\n"
     ]
    }
   ],
   "source": [
    "labels_df = pd.read_csv(csv_path)\n",
    "print(labels_df.head())\n",
    "\n",
    "label_to_int = {label: idx for idx, label in enumerate(labels_df['label'].unique())}\n",
    "int_to_label = {idx: label for label, idx in label_to_int.items()}\n",
    "\n",
    "print(label_to_int)\n",
    "print(int_to_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "torch.manual_seed(random_seed);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = labels_df['id'].apply(lambda x: os.path.join(image_dir, f\"{x}.png\"))\n",
    "labels = labels_df['label'].apply(lambda x: label_to_int[x])    # convert labels to int values according to label_to_int dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    data/cifar10/original_train/train\\1.png\n",
      "1    data/cifar10/original_train/train\\2.png\n",
      "2    data/cifar10/original_train/train\\3.png\n",
      "3    data/cifar10/original_train/train\\4.png\n",
      "4    data/cifar10/original_train/train\\5.png\n",
      "Name: id, dtype: object\n",
      "0    0\n",
      "1    1\n",
      "2    1\n",
      "3    2\n",
      "4    3\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(image_paths.head())\n",
    "print(labels.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Training and Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "class Cifar10Dataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.image_paths.iloc[index])\n",
    "        get_label = self.labels.iloc[index]\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.transform is None:\n",
    "            image = ToTensor()(image)\n",
    "            \n",
    "        label_tensor = torch.tensor(get_label).type(torch.LongTensor)\n",
    "        \n",
    "        return image, label_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "# Data Loaeders for one fold\n",
    "\n",
    "\n",
    "train_dataset = Cifar10Dataset(train_paths, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "mean = 0.0\n",
    "std = 0.0\n",
    "for images, _ in train_loader:\n",
    "    batch_samples = images.size(0)  # batch size (the last batch can have smaller size)\n",
    "    images = images.view(batch_samples, images.size(1), -1)\n",
    "    mean += images.mean(2).sum(0)\n",
    "    std += images.std(2).sum(0)\n",
    "```\n",
    "\n",
    "This code helped me to find the mean and standard deviation of our Cifar10 Dataset, we shall put it in `transofrms.Normalize()` function. By this we can normalize the Tensors accordingly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean /= len(train_loader.dataset)\n",
    "# std /= len(train_loader.dataset)\n",
    "# print(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the image transformation on the training set\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(p=float(random_seed / 100)),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean= [0.4914, 0.4822, 0.4465],\n",
    "                         std= [0.2025, 0.1996, 0.2012]),\n",
    "    transforms.RandomErasing(p=float(random_seed / 100), scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.247, 0.243, 0.261])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_dataset = Cifar10Dataset(image_paths, labels, transform=train_transform)\n",
    "# val_dataset = Cifar10Dataset(valid_paths, valid_labels, transform=valid_transform)\n",
    "transformed_loader = DataLoader(transformed_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=0)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu cpu\n",
      "torch.Size([3, 32, 32]) torch.Size([])\n",
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "for x, y in transformed_dataset:\n",
    "    print(x.device, y.device)\n",
    "    print(x.shape, y.shape)\n",
    "    print(y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(cp.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.71333337..1.6006229].\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "for images, labels in transformed_dataset:\n",
    "    img = images\n",
    "    label = labels\n",
    "    # print(torch.max(img), torch.min(img))\n",
    "    imshow(img)\n",
    "    print(label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Model\n",
    "\n",
    "We will be using a Convolutional Neural Network, with Batch Normalisation, `ReLU` activation function and `Max-Pooling` of 2 x 2 size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define a class for the model to refer to during the training of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cifar10ModelBase(nn.Module):\n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch\n",
    "        preds = self(images)  # Get Predictions\n",
    "        loss = F.cross_entropy(preds, labels)\n",
    "        return loss\n",
    "    \n",
    "    def valid_step(self, batch):\n",
    "        images, labels = batch\n",
    "        preds = self(images)\n",
    "        loss = F.cross_entropy(preds, labels)\n",
    "        acc = accuracy(preds, labels)           # Function to calculate accuracy\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "    \n",
    "    def valid_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()\n",
    "\n",
    "        batch_accuracy = [x['val_acc'] for x in outputs]\n",
    "        epoch_accuracy = torch.stack(batch_accuracy).mean()\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_accuracy.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(f'Epoch [{epoch+1}]: Loss of the Model = {result[\"val_loss\"]:.4f}, Accuracy of the Model = {100 * result[\"val_acc\"]:.2f} %')\n",
    "\n",
    "    \n",
    "def accuracy(outputs, label):\n",
    "    _, preds = torch.max(outputs, dim= 1)   # Get the index of the max log-probability\n",
    "    return torch.tensor(torch.sum(preds == label).item() / len(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use `nn.Sequential` to chain the layers and activations functions into a single network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cifar10Model(Cifar10ModelBase):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            # CONV => BatchNORM => CONV => RELU => POOL => DROPOUT\n",
    "            nn.Conv2d(3, 32, kernel_size= 3, padding= 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(32, 64, kernel_size= 3, padding= 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),     # Output: 64 x 16 x 16\n",
    "            nn.Dropout2d(0.2),\n",
    "\n",
    "            # CONV => BatchNORM => CONV => RELU => POOL => DROPOUT\n",
    "            nn.Conv2d(64, 128, kernel_size= 3, stride= 1, padding= 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(128, 256, kernel_size= 3, stride= 1, padding= 1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.AdaptiveAvgPool2d((16, 16)),     # Output: 128 x 16 x 16\n",
    "            nn.Dropout2d(0.2),\n",
    "\n",
    "            # CONV => BatchNORM => CONV => RELU => POOL => DROPOUT\n",
    "            nn.Conv2d(256, 128, kernel_size= 3, stride= 1, padding= 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(128, 64, kernel_size= 3, stride= 1, padding= 1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(2, 2),     # Output: 64 * 8 * 8\n",
    "            nn.Dropout2d(0.2),\n",
    "\n",
    "            nn.Flatten(),           # Flatten the output\n",
    "            nn.Linear(64 * 8 * 8, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return self.network(xb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that the model produces the expected output on a batch of training data. The 10 outputs for each image can be interpreted as probabilities for the 10 target classes (after applying softmax), and the class with the highest probability is chosen as the label predicted by the model for the input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 32, 32]             896\n",
      "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
      "            Conv2d-3           [-1, 64, 32, 32]          18,496\n",
      "              ReLU-4           [-1, 64, 32, 32]               0\n",
      "         MaxPool2d-5           [-1, 64, 16, 16]               0\n",
      "         Dropout2d-6           [-1, 64, 16, 16]               0\n",
      "            Conv2d-7          [-1, 128, 16, 16]          73,856\n",
      "       BatchNorm2d-8          [-1, 128, 16, 16]             256\n",
      "            Conv2d-9          [-1, 256, 16, 16]         295,168\n",
      "        LeakyReLU-10          [-1, 256, 16, 16]               0\n",
      "AdaptiveAvgPool2d-11          [-1, 256, 16, 16]               0\n",
      "        Dropout2d-12          [-1, 256, 16, 16]               0\n",
      "           Conv2d-13          [-1, 128, 16, 16]         295,040\n",
      "      BatchNorm2d-14          [-1, 128, 16, 16]             256\n",
      "           Conv2d-15           [-1, 64, 16, 16]          73,792\n",
      "        LeakyReLU-16           [-1, 64, 16, 16]               0\n",
      "        MaxPool2d-17             [-1, 64, 8, 8]               0\n",
      "        Dropout2d-18             [-1, 64, 8, 8]               0\n",
      "          Flatten-19                 [-1, 4096]               0\n",
      "           Linear-20                  [-1, 512]       2,097,664\n",
      "             ReLU-21                  [-1, 512]               0\n",
      "          Dropout-22                  [-1, 512]               0\n",
      "           Linear-23                   [-1, 10]           5,130\n",
      "================================================================\n",
      "Total params: 2,860,618\n",
      "Trainable params: 2,860,618\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 5.11\n",
      "Params size (MB): 10.91\n",
      "Estimated Total Size (MB): 16.03\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = Cifar10Model().cuda()\n",
    "# print(model)\n",
    "summary(model, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving to GPU\n",
    "\n",
    "We have made our model to run on the GPU, by calling the class object with a `.cuda()` parameter. Now we move our data to the GPU as-well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader(DataLoader):\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dataset, device, SAMPLER=None):\n",
    "        self.dl = DataLoader(dataset, batch_size=BATCH_SIZE, num_workers=0, sampler=SAMPLER)\n",
    "        self.device = device\n",
    "        super().__init__(self)\n",
    "        # self.batch = dataset\n",
    "        \n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_loader_gpu = DeviceDataLoader(transformed_dataset, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training The Model!!\n",
    "\n",
    "We define a `fit` and `evaluate` function to train the model using Stochastic Gradient Descent and evaluate its perfromance based on the validaton data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader):\n",
    "    with torch.no_grad():\n",
    "        outputs = [model.valid_step(batch) for batch in val_loader]\n",
    "        return model.valid_epoch_end(outputs)\n",
    "    \n",
    "def fit(epochs, model, train_loader, valid_loader, lr=0.01, opt_func= torch.optim.SGD, getScheduler=None):\n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Validation\n",
    "        result = evaluate(model, valid_loader)\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "        if getScheduler is not None:\n",
    "            val_loss = sum(result['val_loss']) / len(valid_loader)\n",
    "            getScheduler.step(val_loss)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model.parameters()).is_cuda    # Check if the model is on the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin with training, let's first evaluate the model before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images.shape: torch.Size([128, 3, 32, 32])\n",
      "cuda:0 cuda:0\n",
      "out.shape: torch.Size([128, 10])\n",
      "Labels shape: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in transformed_loader_gpu:\n",
    "    print('images.shape:', images.shape)\n",
    "    out = model(images)\n",
    "    print(images.device, labels.device)\n",
    "    print('out.shape:', out.shape)\n",
    "    print(f'Labels shape: {labels.shape}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': 2.3307836055755615, 'val_acc': 0.09847746044397354}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, transformed_loader_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arnav Waghdhare\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Define optimizer and scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.1, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "Setting up K-Fold Cross Validation from sklearn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "kf = KFold(n_splits=k, random_state=random_seed, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the Data\n",
    "\n",
    "* Using the folds to create train and valid splits\n",
    "* Map ID's to image paths "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Fold Cross Validation \n",
    "\n",
    "Let's apply K-Fold cross validation on our testing dataset. We will divide the *50,000* images in to *5 folds*, so each fold will have *10,000* images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "------------------\n",
      "Epoch [1]: Loss of the Model = 1.8160, Accuracy of the Model = 32.98 %\n",
      "Epoch [2]: Loss of the Model = 1.6943, Accuracy of the Model = 37.20 %\n",
      "Epoch [3]: Loss of the Model = 1.5948, Accuracy of the Model = 42.34 %\n",
      "Epoch [4]: Loss of the Model = 1.5089, Accuracy of the Model = 46.11 %\n",
      "Epoch [5]: Loss of the Model = 1.4739, Accuracy of the Model = 47.15 %\n",
      "Epoch [6]: Loss of the Model = 1.4298, Accuracy of the Model = 49.09 %\n",
      "Epoch [7]: Loss of the Model = 1.3736, Accuracy of the Model = 50.96 %\n",
      "Epoch [8]: Loss of the Model = 1.3729, Accuracy of the Model = 50.73 %\n",
      "Epoch [9]: Loss of the Model = 1.3086, Accuracy of the Model = 53.30 %\n",
      "Epoch [10]: Loss of the Model = 1.2810, Accuracy of the Model = 54.07 %\n",
      "Epoch [11]: Loss of the Model = 1.2659, Accuracy of the Model = 54.38 %\n",
      "Epoch [12]: Loss of the Model = 1.2488, Accuracy of the Model = 55.62 %\n",
      "Epoch [13]: Loss of the Model = 1.2217, Accuracy of the Model = 56.57 %\n",
      "Epoch [14]: Loss of the Model = 1.2395, Accuracy of the Model = 56.38 %\n",
      "Epoch [15]: Loss of the Model = 1.1882, Accuracy of the Model = 58.10 %\n",
      "Epoch [16]: Loss of the Model = 1.1859, Accuracy of the Model = 58.29 %\n",
      "Epoch [17]: Loss of the Model = 1.1830, Accuracy of the Model = 58.82 %\n",
      "Epoch [18]: Loss of the Model = 1.1452, Accuracy of the Model = 59.49 %\n",
      "Epoch [19]: Loss of the Model = 1.1489, Accuracy of the Model = 60.03 %\n",
      "Epoch [20]: Loss of the Model = 1.1177, Accuracy of the Model = 61.11 %\n",
      "Epoch [21]: Loss of the Model = 1.1046, Accuracy of the Model = 61.35 %\n",
      "Epoch [22]: Loss of the Model = 1.1118, Accuracy of the Model = 60.99 %\n",
      "Epoch [23]: Loss of the Model = 1.1123, Accuracy of the Model = 61.33 %\n",
      "Epoch [24]: Loss of the Model = 1.0981, Accuracy of the Model = 61.99 %\n",
      "Epoch [25]: Loss of the Model = 1.0787, Accuracy of the Model = 62.17 %\n",
      "Epoch [26]: Loss of the Model = 1.0699, Accuracy of the Model = 62.42 %\n",
      "Epoch [27]: Loss of the Model = 1.0505, Accuracy of the Model = 63.44 %\n",
      "Epoch [28]: Loss of the Model = 1.0655, Accuracy of the Model = 63.37 %\n",
      "Epoch [29]: Loss of the Model = 1.0249, Accuracy of the Model = 64.52 %\n",
      "Epoch [30]: Loss of the Model = 1.0251, Accuracy of the Model = 65.22 %\n",
      "------------------\n",
      "Minimum Validation Loss: 1.0248569250106812\n",
      "Highest Accuracy: 0.652195394039154\n",
      "Best model for fold 1 saved with accuracy: 0.65%\n",
      "------------------\n",
      "Fold 2/5\n",
      "------------------\n",
      "Epoch [1]: Loss of the Model = 1.0112, Accuracy of the Model = 64.71 %\n",
      "Epoch [2]: Loss of the Model = 0.9848, Accuracy of the Model = 65.83 %\n",
      "Epoch [3]: Loss of the Model = 0.9805, Accuracy of the Model = 66.03 %\n",
      "Epoch [4]: Loss of the Model = 0.9843, Accuracy of the Model = 66.17 %\n",
      "Epoch [5]: Loss of the Model = 0.9666, Accuracy of the Model = 66.47 %\n",
      "Epoch [6]: Loss of the Model = 0.9951, Accuracy of the Model = 66.25 %\n",
      "Epoch [7]: Loss of the Model = 0.9734, Accuracy of the Model = 66.30 %\n",
      "Epoch [8]: Loss of the Model = 0.9410, Accuracy of the Model = 67.41 %\n",
      "Epoch [9]: Loss of the Model = 0.9494, Accuracy of the Model = 67.53 %\n",
      "Epoch [10]: Loss of the Model = 0.9750, Accuracy of the Model = 66.15 %\n",
      "Epoch [11]: Loss of the Model = 0.9652, Accuracy of the Model = 67.07 %\n",
      "Epoch [12]: Loss of the Model = 0.9354, Accuracy of the Model = 67.94 %\n",
      "Epoch [13]: Loss of the Model = 0.9432, Accuracy of the Model = 67.06 %\n",
      "Epoch [14]: Loss of the Model = 0.9441, Accuracy of the Model = 67.22 %\n",
      "Epoch [15]: Loss of the Model = 0.9276, Accuracy of the Model = 67.40 %\n",
      "Epoch [16]: Loss of the Model = 0.9205, Accuracy of the Model = 68.18 %\n",
      "Epoch [17]: Loss of the Model = 0.9285, Accuracy of the Model = 67.62 %\n",
      "Epoch [18]: Loss of the Model = 0.9179, Accuracy of the Model = 68.79 %\n",
      "Epoch [19]: Loss of the Model = 0.9371, Accuracy of the Model = 67.77 %\n",
      "Epoch [20]: Loss of the Model = 0.9203, Accuracy of the Model = 68.44 %\n",
      "Epoch [21]: Loss of the Model = 0.9154, Accuracy of the Model = 68.47 %\n",
      "Epoch [22]: Loss of the Model = 0.9061, Accuracy of the Model = 68.69 %\n",
      "Epoch [23]: Loss of the Model = 0.8993, Accuracy of the Model = 68.70 %\n",
      "Epoch [24]: Loss of the Model = 0.9266, Accuracy of the Model = 68.65 %\n",
      "Epoch [25]: Loss of the Model = 0.9080, Accuracy of the Model = 69.00 %\n",
      "Epoch [26]: Loss of the Model = 0.9076, Accuracy of the Model = 68.39 %\n",
      "Epoch [27]: Loss of the Model = 0.9058, Accuracy of the Model = 68.70 %\n",
      "Epoch [28]: Loss of the Model = 0.8728, Accuracy of the Model = 69.75 %\n",
      "Epoch [29]: Loss of the Model = 0.8974, Accuracy of the Model = 68.85 %\n",
      "Epoch [30]: Loss of the Model = 0.8961, Accuracy of the Model = 68.98 %\n",
      "------------------\n",
      "Minimum Validation Loss: 0.8728252649307251\n",
      "Highest Accuracy: 0.6974881291389465\n",
      "Best model for fold 2 saved with accuracy: 0.70%\n",
      "------------------\n",
      "Fold 3/5\n",
      "------------------\n",
      "Epoch [1]: Loss of the Model = 0.8076, Accuracy of the Model = 72.11 %\n",
      "Epoch [2]: Loss of the Model = 0.8120, Accuracy of the Model = 72.52 %\n",
      "Epoch [3]: Loss of the Model = 0.8128, Accuracy of the Model = 71.82 %\n",
      "Epoch [4]: Loss of the Model = 0.8255, Accuracy of the Model = 71.62 %\n",
      "Epoch [5]: Loss of the Model = 0.8282, Accuracy of the Model = 71.25 %\n",
      "Epoch [6]: Loss of the Model = 0.8227, Accuracy of the Model = 71.88 %\n",
      "Epoch [7]: Loss of the Model = 0.8143, Accuracy of the Model = 72.35 %\n",
      "Epoch [8]: Loss of the Model = 0.8082, Accuracy of the Model = 72.26 %\n",
      "Epoch [9]: Loss of the Model = 0.8139, Accuracy of the Model = 72.13 %\n",
      "Epoch [10]: Loss of the Model = 0.8088, Accuracy of the Model = 72.05 %\n",
      "Epoch [11]: Loss of the Model = 0.8174, Accuracy of the Model = 71.72 %\n",
      "Epoch [12]: Loss of the Model = 0.8166, Accuracy of the Model = 72.64 %\n",
      "Epoch [13]: Loss of the Model = 0.8047, Accuracy of the Model = 72.16 %\n",
      "Epoch [14]: Loss of the Model = 0.7895, Accuracy of the Model = 72.70 %\n",
      "Epoch [15]: Loss of the Model = 0.7918, Accuracy of the Model = 72.87 %\n",
      "Epoch [16]: Loss of the Model = 0.8045, Accuracy of the Model = 72.27 %\n",
      "Epoch [17]: Loss of the Model = 0.8082, Accuracy of the Model = 71.88 %\n",
      "Epoch [18]: Loss of the Model = 0.7995, Accuracy of the Model = 72.30 %\n",
      "Epoch [19]: Loss of the Model = 0.8097, Accuracy of the Model = 71.80 %\n",
      "Epoch [20]: Loss of the Model = 0.8093, Accuracy of the Model = 72.43 %\n",
      "Epoch [21]: Loss of the Model = 0.7920, Accuracy of the Model = 72.97 %\n",
      "Epoch [22]: Loss of the Model = 0.7939, Accuracy of the Model = 72.69 %\n",
      "Epoch [23]: Loss of the Model = 0.8047, Accuracy of the Model = 72.20 %\n",
      "Epoch [24]: Loss of the Model = 0.7794, Accuracy of the Model = 73.74 %\n",
      "Epoch [25]: Loss of the Model = 0.7964, Accuracy of the Model = 72.55 %\n",
      "Epoch [26]: Loss of the Model = 0.8103, Accuracy of the Model = 72.70 %\n",
      "Epoch [27]: Loss of the Model = 0.8033, Accuracy of the Model = 72.43 %\n",
      "Epoch [28]: Loss of the Model = 0.7877, Accuracy of the Model = 72.69 %\n",
      "Epoch [29]: Loss of the Model = 0.8018, Accuracy of the Model = 72.73 %\n",
      "Epoch [30]: Loss of the Model = 0.7861, Accuracy of the Model = 73.05 %\n",
      "------------------\n",
      "Minimum Validation Loss: 0.7794325947761536\n",
      "Highest Accuracy: 0.7374406456947327\n",
      "Best model for fold 3 saved with accuracy: 0.74%\n",
      "------------------\n",
      "Fold 4/5\n",
      "------------------\n",
      "Epoch [1]: Loss of the Model = 0.7329, Accuracy of the Model = 74.45 %\n",
      "Epoch [2]: Loss of the Model = 0.7336, Accuracy of the Model = 74.84 %\n",
      "Epoch [3]: Loss of the Model = 0.7320, Accuracy of the Model = 74.61 %\n",
      "Epoch [4]: Loss of the Model = 0.7220, Accuracy of the Model = 75.12 %\n",
      "Epoch [5]: Loss of the Model = 0.7210, Accuracy of the Model = 74.66 %\n",
      "Epoch [6]: Loss of the Model = 0.7242, Accuracy of the Model = 75.05 %\n",
      "Epoch [7]: Loss of the Model = 0.7220, Accuracy of the Model = 75.39 %\n",
      "Epoch [8]: Loss of the Model = 0.7265, Accuracy of the Model = 74.68 %\n",
      "Epoch [9]: Loss of the Model = 0.7284, Accuracy of the Model = 74.88 %\n",
      "Epoch [10]: Loss of the Model = 0.7393, Accuracy of the Model = 74.38 %\n",
      "Epoch [11]: Loss of the Model = 0.7158, Accuracy of the Model = 75.43 %\n",
      "Epoch [12]: Loss of the Model = 0.7251, Accuracy of the Model = 75.65 %\n",
      "Epoch [13]: Loss of the Model = 0.7270, Accuracy of the Model = 75.00 %\n",
      "Epoch [14]: Loss of the Model = 0.7195, Accuracy of the Model = 75.26 %\n",
      "Epoch [15]: Loss of the Model = 0.7225, Accuracy of the Model = 75.04 %\n",
      "Epoch [16]: Loss of the Model = 0.7239, Accuracy of the Model = 75.24 %\n",
      "Epoch [17]: Loss of the Model = 0.7056, Accuracy of the Model = 75.81 %\n",
      "Epoch [18]: Loss of the Model = 0.7145, Accuracy of the Model = 75.17 %\n",
      "Epoch [19]: Loss of the Model = 0.7141, Accuracy of the Model = 75.67 %\n",
      "Epoch [20]: Loss of the Model = 0.7247, Accuracy of the Model = 75.24 %\n",
      "Epoch [21]: Loss of the Model = 0.7062, Accuracy of the Model = 75.45 %\n",
      "Epoch [22]: Loss of the Model = 0.7158, Accuracy of the Model = 75.06 %\n",
      "Epoch [23]: Loss of the Model = 0.7113, Accuracy of the Model = 75.94 %\n",
      "Epoch [24]: Loss of the Model = 0.7147, Accuracy of the Model = 75.13 %\n",
      "Epoch [25]: Loss of the Model = 0.7177, Accuracy of the Model = 75.53 %\n",
      "Epoch [26]: Loss of the Model = 0.7307, Accuracy of the Model = 75.25 %\n",
      "Epoch [27]: Loss of the Model = 0.7264, Accuracy of the Model = 74.84 %\n",
      "Epoch [28]: Loss of the Model = 0.7054, Accuracy of the Model = 75.77 %\n",
      "Epoch [29]: Loss of the Model = 0.7171, Accuracy of the Model = 75.61 %\n",
      "Epoch [30]: Loss of the Model = 0.7191, Accuracy of the Model = 75.33 %\n",
      "------------------\n",
      "Minimum Validation Loss: 0.7054290175437927\n",
      "Highest Accuracy: 0.7593947649002075\n",
      "Best model for fold 4 saved with accuracy: 0.76%\n",
      "------------------\n",
      "Fold 5/5\n",
      "------------------\n",
      "Epoch [1]: Loss of the Model = 0.6638, Accuracy of the Model = 77.16 %\n",
      "Epoch [2]: Loss of the Model = 0.6676, Accuracy of the Model = 76.94 %\n",
      "Epoch [3]: Loss of the Model = 0.6742, Accuracy of the Model = 76.85 %\n",
      "Epoch [4]: Loss of the Model = 0.6639, Accuracy of the Model = 76.64 %\n",
      "Epoch [5]: Loss of the Model = 0.6650, Accuracy of the Model = 76.86 %\n",
      "Epoch [6]: Loss of the Model = 0.6741, Accuracy of the Model = 76.50 %\n",
      "Epoch [7]: Loss of the Model = 0.6683, Accuracy of the Model = 76.90 %\n",
      "Epoch [8]: Loss of the Model = 0.6844, Accuracy of the Model = 76.11 %\n",
      "Epoch [9]: Loss of the Model = 0.6670, Accuracy of the Model = 76.72 %\n",
      "Epoch [10]: Loss of the Model = 0.6558, Accuracy of the Model = 77.84 %\n",
      "Epoch [11]: Loss of the Model = 0.6943, Accuracy of the Model = 76.25 %\n",
      "Epoch [12]: Loss of the Model = 0.6770, Accuracy of the Model = 76.58 %\n",
      "Epoch [13]: Loss of the Model = 0.6697, Accuracy of the Model = 77.53 %\n",
      "Epoch [14]: Loss of the Model = 0.6618, Accuracy of the Model = 77.24 %\n",
      "Epoch [15]: Loss of the Model = 0.6697, Accuracy of the Model = 76.95 %\n",
      "Epoch [16]: Loss of the Model = 0.6711, Accuracy of the Model = 76.77 %\n",
      "Epoch [17]: Loss of the Model = 0.6588, Accuracy of the Model = 76.79 %\n",
      "Epoch [18]: Loss of the Model = 0.6721, Accuracy of the Model = 76.87 %\n",
      "Epoch [19]: Loss of the Model = 0.6757, Accuracy of the Model = 76.60 %\n",
      "Epoch [20]: Loss of the Model = 0.6712, Accuracy of the Model = 76.65 %\n",
      "Epoch [21]: Loss of the Model = 0.6785, Accuracy of the Model = 76.88 %\n",
      "Epoch [22]: Loss of the Model = 0.6651, Accuracy of the Model = 77.04 %\n",
      "Epoch [23]: Loss of the Model = 0.6616, Accuracy of the Model = 76.83 %\n",
      "Epoch [24]: Loss of the Model = 0.6856, Accuracy of the Model = 76.45 %\n",
      "Epoch [25]: Loss of the Model = 0.6645, Accuracy of the Model = 77.00 %\n",
      "Epoch [26]: Loss of the Model = 0.6712, Accuracy of the Model = 76.74 %\n",
      "Epoch [27]: Loss of the Model = 0.6619, Accuracy of the Model = 77.19 %\n",
      "Epoch [28]: Loss of the Model = 0.6728, Accuracy of the Model = 76.83 %\n",
      "Epoch [29]: Loss of the Model = 0.6755, Accuracy of the Model = 76.32 %\n",
      "Epoch [30]: Loss of the Model = 0.6728, Accuracy of the Model = 76.53 %\n",
      "------------------\n",
      "Minimum Validation Loss: 0.6558318734169006\n",
      "Highest Accuracy: 0.7783821225166321\n",
      "Best model for fold 5 saved with accuracy: 0.78%\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "k_folds_history = []\n",
    "best_accuracy = 0.0\n",
    "\n",
    "# Stratified K-Fold Cross Validation\n",
    "for fold, (train_idx, valid_idx) in enumerate(kf.split(transformed_dataset)):\n",
    "    print(f\"Fold {fold+1}/{k}\")\n",
    "    print(\"------------------\")\n",
    "\n",
    "    num_epochs = 30\n",
    "    lr = 0.001 + (fold * (-1 * 0.0002))\n",
    "    \n",
    "    # Sample elements randomly from a given list of ids, no replacement.\n",
    "    train_subsampler = SubsetRandomSampler(train_idx)\n",
    "    valid_subsampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "    # Define data loaders for training and testing data in this fold\n",
    "    train_loader = DeviceDataLoader(transformed_dataset, device, train_subsampler)\n",
    "    valid_loader = DeviceDataLoader(transformed_dataset, device, valid_subsampler)\n",
    "\n",
    "    history = fit(num_epochs, model, train_loader, valid_loader, lr=lr, opt_func=torch.optim.AdamW)\n",
    "\n",
    "    k_folds_history.append(history)\n",
    "    print(\"------------------\")\n",
    "    min_val_loss = min(entry['val_loss'] for entry in history)\n",
    "    max_val_acc = max(entry['val_acc'] for entry in history)\n",
    "    print(f'Minimum Validation Loss: {min_val_loss}')\n",
    "    print(f'Highest Accuracy: {max_val_acc}')\n",
    "    if max_val_acc > best_accuracy:\n",
    "            best_accuracy = max_val_acc\n",
    "            model_save_path = f\"model_fold_{fold+1}.pth\"\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print(f\"Best model for fold {fold+1} saved with accuracy: {best_accuracy:.2f}%\")\n",
    "\n",
    "    print(\"------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Model\n",
    "Am going to trianthis over night, so once the process is completed I need to save the model else I would have to spend another hour or so next time to get the fuckin model and run the stuff on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path where the model will be saved\n",
    "model_save_path = \"Cifar10Trained_Lv2.pth\"\n",
    "\n",
    "# Save the model's state dictionary\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "print(f\"Model saved to {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"Cifar10Trained.pth\"))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images_dir = r\"data/cifar10/original_test\"\n",
    "parent_list = sorted(os.listdir(test_images_dir), key=len)\n",
    "\n",
    "# print(parent_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 done\n",
      "Batch 2 done\n",
      "Batch 3 done\n",
      "Batch 4 done\n",
      "Batch 5 done\n",
      "Batch 6 done\n",
      "Batch 7 done\n",
      "Batch 8 done\n",
      "Batch 9 done\n",
      "Batch 10 done\n",
      "Batch 11 done\n",
      "Batch 12 done\n",
      "Batch 13 done\n",
      "Batch 14 done\n",
      "Batch 15 done\n",
      "Batch 16 done\n",
      "Batch 17 done\n",
      "Batch 18 done\n",
      "Batch 19 done\n",
      "Batch 20 done\n",
      "Batch 21 done\n",
      "Batch 22 done\n",
      "Batch 23 done\n",
      "Batch 24 done\n",
      "Batch 25 done\n",
      "Batch 26 done\n",
      "Batch 27 done\n",
      "Batch 28 done\n",
      "Batch 29 done\n",
      "Batch 30 done\n",
      "Finished!!!\n"
     ]
    }
   ],
   "source": [
    "with open('data/cifar10/Submission_Lv2.csv', 'w') as file:\n",
    "    file.write(\"id,label\\n\")\n",
    "    for img_path in parent_list:\n",
    "        # print(img_path[1])\n",
    "        image = os.path.join(test_images_dir, img_path)\n",
    "        image = Image.open(image)\n",
    "        image = valid_transform(image)\n",
    "        image = image.unsqueeze(0)\n",
    "        image = image.to(device)\n",
    "\n",
    "        output = model(image)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        predicted = int_to_label[predicted.item()]\n",
    "        # print(\"Prediction Completed!\")\n",
    "\n",
    "        num = int(img_path.split('.')[0])\n",
    "        file.write(f\"{num},{predicted}\\n\")\n",
    "        \n",
    "        if num % 10000 == 0:\n",
    "            print(f\"Batch {num // 10000} done\")\n",
    "        if num == 300000:\n",
    "            print(\"Finished!!!\")\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
