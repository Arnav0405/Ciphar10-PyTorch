{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x161692da3c0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "# import numpy as np\n",
    "import cupy as cp\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from random import random\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.device_count()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Data\n",
    "Understanding the file structure and the structuring of the given dataset by kaggle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "IMG_HEIGHT = 32\n",
    "IMG_WIDTH = 32\n",
    "RANDOM_SEED = 28\n",
    "\n",
    "root = Path(\"data/cifar10\")\n",
    "\n",
    "# (root/'train').rename(root/'original_train')\n",
    "# (root/'test').rename(root/'original_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = r\"data/cifar10/original_train/train\"\n",
    "csv_path = r\"data/cifar10/trainLabels.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id       label\n",
      "0   1        frog\n",
      "1   2       truck\n",
      "2   3       truck\n",
      "3   4        deer\n",
      "4   5  automobile\n",
      "{'frog': 0, 'truck': 1, 'deer': 2, 'automobile': 3, 'bird': 4, 'horse': 5, 'ship': 6, 'cat': 7, 'dog': 8, 'airplane': 9}\n",
      "{0: 'frog', 1: 'truck', 2: 'deer', 3: 'automobile', 4: 'bird', 5: 'horse', 6: 'ship', 7: 'cat', 8: 'dog', 9: 'airplane'}\n"
     ]
    }
   ],
   "source": [
    "labels_df = pd.read_csv(csv_path)\n",
    "print(labels_df.head())\n",
    "\n",
    "label_to_int = {label: idx for idx, label in enumerate(labels_df['label'].unique())}\n",
    "int_to_label = {idx: label for label, idx in label_to_int.items()}\n",
    "\n",
    "print(label_to_int)\n",
    "print(int_to_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "torch.manual_seed(random_seed);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = labels_df['id'].apply(lambda x: os.path.join(image_dir, f\"{x}.png\"))\n",
    "labels = labels_df['label'].apply(lambda x: label_to_int[x])    # convert labels to int values according to label_to_int dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    data/cifar10/original_train/train\\1.png\n",
      "1    data/cifar10/original_train/train\\2.png\n",
      "2    data/cifar10/original_train/train\\3.png\n",
      "3    data/cifar10/original_train/train\\4.png\n",
      "4    data/cifar10/original_train/train\\5.png\n",
      "Name: id, dtype: object\n",
      "0    0\n",
      "1    1\n",
      "2    1\n",
      "3    2\n",
      "4    3\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(image_paths.head())\n",
    "print(labels.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Training and Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "class Cifar10Dataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.image_paths.iloc[index])\n",
    "        get_label = self.labels.iloc[index]\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.transform is None:\n",
    "            image = ToTensor()(image)\n",
    "            \n",
    "        label_tensor = torch.tensor(get_label).type(torch.LongTensor)\n",
    "        \n",
    "        return image, label_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "# Data Loaeders for one fold\n",
    "\n",
    "\n",
    "train_dataset = Cifar10Dataset(train_paths, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "mean = 0.0\n",
    "std = 0.0\n",
    "for images, _ in train_loader:\n",
    "    batch_samples = images.size(0)  # batch size (the last batch can have smaller size)\n",
    "    images = images.view(batch_samples, images.size(1), -1)\n",
    "    mean += images.mean(2).sum(0)\n",
    "    std += images.std(2).sum(0)\n",
    "```\n",
    "\n",
    "This code helped me to find the mean and standard deviation of our Cifar10 Dataset, we shall put it in `transofrms.Normalize()` function. By this we can normalize the Tensors accordingly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean /= len(train_loader.dataset)\n",
    "# std /= len(train_loader.dataset)\n",
    "# print(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the image transformation on the training set\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(p=float(random_seed / 100)),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean= [0.4914, 0.4822, 0.4465],\n",
    "                         std= [0.2025, 0.1996, 0.2012]),\n",
    "    transforms.RandomErasing(p=float(random_seed / 100), scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.247, 0.243, 0.261])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_dataset = Cifar10Dataset(image_paths, labels, transform=train_transform)\n",
    "# val_dataset = Cifar10Dataset(valid_paths, valid_labels, transform=valid_transform)\n",
    "transformed_loader = DataLoader(transformed_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=0)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu cpu\n",
      "torch.Size([3, 32, 32]) torch.Size([])\n",
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "for x, y in transformed_dataset:\n",
    "    print(x.device, y.device)\n",
    "    print(x.shape, y.shape)\n",
    "    print(y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(cp.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.71333337..1.6006229].\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "for images, labels in transformed_dataset:\n",
    "    img = images\n",
    "    label = labels\n",
    "    # print(torch.max(img), torch.min(img))\n",
    "    imshow(img)\n",
    "    print(label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Model\n",
    "\n",
    "We will be using a Convolutional Neural Network, with Batch Normalisation, `ReLU` activation function and `Max-Pooling` of 2 x 2 size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define a class for the model to refer to during the training of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cifar10ModelBase(nn.Module):\n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch\n",
    "        preds = self(images)  # Get Predictions\n",
    "        loss = F.cross_entropy(preds, labels)\n",
    "        return loss\n",
    "    \n",
    "    def valid_step(self, batch):\n",
    "        images, labels = batch\n",
    "        preds = self(images)\n",
    "        loss = F.cross_entropy(preds, labels)\n",
    "        acc = accuracy(preds, labels)           # Function to calculate accuracy\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "    \n",
    "    def valid_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()\n",
    "\n",
    "        batch_accuracy = [x['val_acc'] for x in outputs]\n",
    "        epoch_accuracy = torch.stack(batch_accuracy).mean()\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_accuracy.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(f'Epoch [{epoch+1}]: Loss of the Model = {result[\"val_loss\"]:.4f}, Accuracy of the Model = {100 * result[\"val_acc\"]:.2f} %')\n",
    "\n",
    "    \n",
    "def accuracy(outputs, label):\n",
    "    _, preds = torch.max(outputs, dim= 1)   # Get the index of the max log-probability\n",
    "    return torch.tensor(torch.sum(preds == label).item() / len(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use `nn.Sequential` to chain the layers and activations functions into a single network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cifar10Model(Cifar10ModelBase):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            # CONV => BatchNORM => CONV => RELU => POOL => DROPOUT\n",
    "            nn.Conv2d(3, 32, kernel_size= 3, padding= 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(32, 64, kernel_size= 3, padding= 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),     # Output: 64 x 16 x 16\n",
    "            nn.Dropout2d(0.2),\n",
    "\n",
    "            # CONV => BatchNORM => CONV => RELU => POOL => DROPOUT\n",
    "            nn.Conv2d(64, 128, kernel_size= 3, stride= 1, padding= 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(128, 256, kernel_size= 3, stride= 1, padding= 1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.AdaptiveAvgPool2d((256, 256)),     # Output: 128 x 8 x 8\n",
    "            nn.Dropout2d(0.2),\n",
    "\n",
    "            # CONV => BatchNORM => CONV => RELU => POOL => DROPOUT\n",
    "            nn.Conv2d(256, 128, kernel_size= 3, stride= 1, padding= 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(128, 64, kernel_size= 3, stride= 1, padding= 1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(2, 2),     # Output: 128 x 4 x 4\n",
    "            nn.Dropout2d(0.2),\n",
    "\n",
    "            nn.Flatten(),           # Flatten the output\n",
    "            nn.Linear(64*4*4, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return self.network(xb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that the model produces the expected output on a batch of training data. The 10 outputs for each image can be interpreted as probabilities for the 10 target classes (after applying softmax), and the class with the highest probability is chosen as the label predicted by the model for the input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2x1048576 and 1024x512)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m Cifar10Model()\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# print(model)\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torchsummary\\torchsummary.py:72\u001b[0m, in \u001b[0;36msummary\u001b[1;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[0;32m     68\u001b[0m model\u001b[38;5;241m.\u001b[39mapply(register_hook)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# make a forward pass\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# print(x.shape)\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# remove these hooks\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m hooks:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[71], line 38\u001b[0m, in \u001b[0;36mCifar10Model.forward\u001b[1;34m(self, xb)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, xb):\n\u001b[1;32m---> 38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1844\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[0;32m   1843\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1844\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1846\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[0;32m   1847\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[0;32m   1848\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[0;32m   1849\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1790\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1787\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[0;32m   1788\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[1;32m-> 1790\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1791\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[0;32m   1792\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[0;32m   1793\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[0;32m   1794\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[0;32m   1795\u001b[0m     ):\n\u001b[0;32m   1796\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2x1048576 and 1024x512)"
     ]
    }
   ],
   "source": [
    "model = Cifar10Model().cuda()\n",
    "# print(model)\n",
    "summary(model, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving to GPU\n",
    "\n",
    "We have made our model to run on the GPU, by calling the class object with a `.cuda()` parameter. Now we move our data to the GPU as-well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader(DataLoader):\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dataset, device, SAMPLER=None):\n",
    "        self.dl = DataLoader(dataset, batch_size=BATCH_SIZE, num_workers=0, sampler=SAMPLER)\n",
    "        self.device = device\n",
    "        super().__init__(self)\n",
    "        # self.batch = dataset\n",
    "        \n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_loader_gpu = DeviceDataLoader(transformed_dataset, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training The Model!!\n",
    "\n",
    "We define a `fit` and `evaluate` function to train the model using Stochastic Gradient Descent and evaluate its perfromance based on the validaton data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader):\n",
    "    with torch.no_grad():\n",
    "        outputs = [model.valid_step(batch) for batch in val_loader]\n",
    "        return model.valid_epoch_end(outputs)\n",
    "    \n",
    "def fit(epochs, model, train_loader, valid_loader, lr=0.01, opt_func= torch.optim.SGD, getScheduler=None):\n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Validation\n",
    "        result = evaluate(model, valid_loader)\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "        if getScheduler is not None:\n",
    "            val_loss = sum(result['val_loss']) / len(valid_loader)\n",
    "            getScheduler.step(val_loss)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model.parameters()).is_cuda    # Check if the model is on the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin with training, let's first evaluate the model before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images.shape: torch.Size([128, 3, 32, 32])\n",
      "out.shape: torch.Size([128, 10])\n",
      "Labels shape: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in transformed_loader_gpu:\n",
    "    print('images.shape:', images.shape)\n",
    "    out = model(images)\n",
    "    print(images.device, labels.device)\n",
    "    print('out.shape:', out.shape)\n",
    "    print(f'Labels shape: {labels.shape}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': 2.3084659576416016, 'val_acc': 0.09522458165884018}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, transformed_loader_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arnav Waghdhare\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Define optimizer and scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.1, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "Setting up K-Fold Cross Validation from sklearn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "kf = KFold(n_splits=k, random_state=random_seed, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the Data\n",
    "\n",
    "* Using the folds to create train and valid splits\n",
    "* Map ID's to image paths "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Fold Cross Validation \n",
    "\n",
    "Let's apply K-Fold cross validation on our testing dataset. We will divide the *50,000* images in to *5 folds*, so each fold will have *10,000* images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "------------------\n",
      "Epoch [1]: Loss of the Model = 1.6606, Accuracy of the Model = 38.97 %\n",
      "Epoch [2]: Loss of the Model = 1.5146, Accuracy of the Model = 45.48 %\n",
      "Epoch [3]: Loss of the Model = 1.3743, Accuracy of the Model = 50.62 %\n",
      "Epoch [4]: Loss of the Model = 1.3013, Accuracy of the Model = 53.47 %\n",
      "Epoch [5]: Loss of the Model = 1.2302, Accuracy of the Model = 56.28 %\n",
      "Epoch [6]: Loss of the Model = 1.2197, Accuracy of the Model = 56.01 %\n",
      "Epoch [7]: Loss of the Model = 1.1931, Accuracy of the Model = 58.13 %\n",
      "Epoch [8]: Loss of the Model = 1.1245, Accuracy of the Model = 60.45 %\n",
      "Epoch [9]: Loss of the Model = 1.1172, Accuracy of the Model = 60.72 %\n",
      "Epoch [10]: Loss of the Model = 1.0889, Accuracy of the Model = 61.38 %\n",
      "Epoch [11]: Loss of the Model = 1.0704, Accuracy of the Model = 62.61 %\n",
      "Epoch [12]: Loss of the Model = 1.0458, Accuracy of the Model = 63.94 %\n",
      "Epoch [13]: Loss of the Model = 1.0370, Accuracy of the Model = 63.73 %\n",
      "Epoch [14]: Loss of the Model = 1.0297, Accuracy of the Model = 64.52 %\n",
      "Epoch [15]: Loss of the Model = 0.9606, Accuracy of the Model = 66.52 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DeviceDataLoader(transformed_dataset, device, train_subsampler)\n\u001b[0;32m     18\u001b[0m valid_loader \u001b[38;5;241m=\u001b[39m DeviceDataLoader(transformed_dataset, device, valid_subsampler)\n\u001b[1;32m---> 20\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m k_folds_history\u001b[38;5;241m.\u001b[39mappend(history)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[23], line 12\u001b[0m, in \u001b[0;36mfit\u001b[1;34m(epochs, model, train_loader, valid_loader, lr, opt_func)\u001b[0m\n\u001b[0;32m      8\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m opt_func(model\u001b[38;5;241m.\u001b[39mparameters(), lr)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[21], line 18\u001b[0m, in \u001b[0;36mDeviceDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     17\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Yield a batch of data after moving it to device\"\"\"\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdl\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mto_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[10], line 17\u001b[0m, in \u001b[0;36mCifar10Dataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     14\u001b[0m get_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels\u001b[38;5;241m.\u001b[39miloc[index]\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 17\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     20\u001b[0m     image \u001b[38;5;241m=\u001b[39m ToTensor()(image)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "k_folds_history = []\n",
    "best_accuracy = 0.0\n",
    "\n",
    "# Stratified K-Fold Cross Validation\n",
    "for fold, (train_idx, valid_idx) in enumerate(kf.split(transformed_dataset)):\n",
    "    print(f\"Fold {fold+1}/{k}\")\n",
    "    print(\"------------------\")\n",
    "\n",
    "    num_epochs = 25\n",
    "\n",
    "    \n",
    "    # Sample elements randomly from a given list of ids, no replacement.\n",
    "    train_subsampler = SubsetRandomSampler(train_idx)\n",
    "    valid_subsampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "    # Define data loaders for training and testing data in this fold\n",
    "    train_loader = DeviceDataLoader(transformed_dataset, device, train_subsampler)\n",
    "    valid_loader = DeviceDataLoader(transformed_dataset, device, valid_subsampler)\n",
    "\n",
    "    history = fit(num_epochs, model, train_loader, valid_loader, lr=lr, opt_func=torch.optim.AdamW)\n",
    "\n",
    "    k_folds_history.append(history)\n",
    "    print(\"------------------\")\n",
    "    min_val_loss = min(entry['val_loss'] for entry in history)\n",
    "    max_val_acc = max(entry['val_acc'] for entry in history)\n",
    "    print(f'Minimum Validation Loss: {min_val_loss}')\n",
    "    print(f'Highest Accuracy: {max_val_acc}')\n",
    "    if max_val_acc > best_accuracy:\n",
    "            best_accuracy = max_val_acc\n",
    "            model_save_path = f\"model_fold_{fold+1}.pth\"\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print(f\"Best model for fold {fold+1} saved with accuracy: {best_accuracy:.2f}%\")\n",
    "\n",
    "    print(\"------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Model\n",
    "Am going to trianthis over night, so once the process is completed I need to save the model else I would have to spend another hour or so next time to get the fuckin model and run the stuff on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path where the model will be saved\n",
    "model_save_path = \"Cifar10Trained_Lv2.pth\"\n",
    "\n",
    "# Save the model's state dictionary\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "print(f\"Model saved to {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"Cifar10Trained.pth\"))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images_dir = r\"data/cifar10/original_test\"\n",
    "parent_list = sorted(os.listdir(test_images_dir), key=len)\n",
    "\n",
    "# print(parent_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/cifar10/Submission_Lv2.csv', 'w') as file:\n",
    "    file.write(\"id,label\\n\")\n",
    "    for img_path in parent_list:\n",
    "        # print(img_path[1])\n",
    "        image = os.path.join(test_images_dir, img_path)\n",
    "        image = Image.open(image)\n",
    "        image = valid_transform(image)\n",
    "        image = image.unsqueeze(0)\n",
    "        image = image.to(device)\n",
    "\n",
    "        output = model(image)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        predicted = int_to_label[predicted.item()]\n",
    "        # print(\"Prediction Completed!\")\n",
    "\n",
    "        num = int(img_path.split('.')[0])\n",
    "        file.write(f\"{num},{predicted}\\n\")\n",
    "        \n",
    "        if num % 10000 == 0:\n",
    "            print(f\"Batch {num // 10000} done\")\n",
    "        if num == 300000:\n",
    "            print(\"Finished!!!\")\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
